{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3436986c-5d7d-4b32-8bac-791103139967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchdiffeq\n",
    "import torchsde\n",
    "import torchvision as tv\n",
    "import tqdm\n",
    "import wandb\n",
    "import unet\n",
    "from fractional_noise import SparseGPNoise\n",
    "from torch import nn, optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe9eb26-3f89-461b-9b69-c999ef9c6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_tail_dims(y: torch.Tensor, y_like: torch.Tensor):\n",
    "    \"\"\"Fill in missing trailing dimensions for y according to y_like.\"\"\"\n",
    "    return y[(...,) + (None,) * (y_like.dim() - y.dim())]\n",
    "\n",
    "\n",
    "class Module(abc.ABC, nn.Module):\n",
    "    \"\"\"A wrapper module that's more convenient to use.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "        self._checkpoint = False\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for p in self.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "class ScoreMatchingSDE(Module):\n",
    "    \"\"\"Wraps score network with analytical sampling and cond. score computation.\n",
    "    The variance preserving formulation in\n",
    "        Score-Based Generative Modeling through Stochastic Differential Equations\n",
    "        https://arxiv.org/abs/2011.13456\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        denoiser,\n",
    "        white_noise,\n",
    "        input_size=(1, 28, 28),\n",
    "        t0=0.0,\n",
    "        t1=1.0,\n",
    "        beta_min=0.1,\n",
    "        beta_max=50.0,\n",
    "    ):\n",
    "        super(ScoreMatchingSDE, self).__init__()\n",
    "        if t0 > t1:\n",
    "            raise ValueError(f\"Expected t0 <= t1, but found t0={t0:.4f}, t1={t1:.4f}\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.denoiser = denoiser\n",
    "        self.white_noise = white_noise\n",
    "\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "\n",
    "        # discretization to compute integral of \\alpha = \\beta * h^2\n",
    "        self.register_buffer(\"steps\", torch.linspace(self.t0, self.t1, 100))\n",
    "\n",
    "    def precompute_white_noise(self, batch_size=1):\n",
    "        \"\"\"Precompute inducing points\"\"\"\n",
    "        self.white_noise.precompute(batch_size)\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"Precommpute Cholesky\"\"\"\n",
    "        self.precompute_white_noise()\n",
    "        var = torch.stack([self.white_noise(t)[1].pow(2) for t in self.steps], dim=0)\n",
    "        #\n",
    "        self._var = var.squeeze().clamp(1e-5)\n",
    "\n",
    "    @property\n",
    "    def white_noise_var(self):\n",
    "        return self._var\n",
    "\n",
    "    def original_f(self, t, y):\n",
    "        return -0.5 * self._beta(t) * y\n",
    "\n",
    "    def original_g(self, t, y):\n",
    "        sqrt_beta = self._beta(t).sqrt()\n",
    "        return fill_tail_dims(sqrt_beta, y).expand_as(y)\n",
    "\n",
    "    def score(self, t, y):\n",
    "        if isinstance(t, float):\n",
    "            t = y.new_tensor(t)\n",
    "        if t.dim() == 0:\n",
    "            t = t.repeat(y.shape[0])\n",
    "        return self.denoiser(t, y)\n",
    "\n",
    "    def _beta(self, t):\n",
    "        return self.beta_min + t * (self.beta_max - self.beta_min)\n",
    "\n",
    "    def _indefinite_int(self, t):\n",
    "        r\"\"\"Indefinite integral.\n",
    "\n",
    "        Note that the fractional case we commpute the integral\n",
    "\n",
    "            \\int \\beta(t)\\varsigma(t) dt\n",
    "        \"\"\"\n",
    "\n",
    "        if not torch.is_tensor(t):\n",
    "            t = self.steps.new_tensor([t])\n",
    "\n",
    "        beta_steps = self._beta(self.steps).unsqueeze(0)  # size: 1, num_step\n",
    "        diff = t.unsqueeze(-1) - self.steps.unsqueeze(0)\n",
    "        indicator = (diff > 0).float()  # size: batch size, num step\n",
    "\n",
    "        ret = indicator * beta_steps * self.white_noise_var.unsqueeze(0)\n",
    "        ret = ret.sum(-1) * (self.steps[1] - self.steps[0])\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def analytical_mean(self, t, x_t0):\n",
    "        r\"\"\"Compute the analytic mean of p(X_t|X_0)\n",
    "\n",
    "        X_0 * exp{-0.5\\int_0^T \\beta(t) \\varsigma(t)dt}\n",
    "        \"\"\"\n",
    "        mean_coeff = (\n",
    "            -0.5 * (self._indefinite_int(t) - self._indefinite_int(self.t0))\n",
    "        ).exp()\n",
    "        mean = x_t0 * fill_tail_dims(mean_coeff, x_t0)\n",
    "        return mean\n",
    "\n",
    "    def analytical_var(self, t, x_t0):\n",
    "        r\"\"\"Compute analytical variance of p(X_t|X_0)\n",
    "\n",
    "        I - I \\exp{-\\int_0^T \\beta(t) \\varsigma(t) dt}\n",
    "        \"\"\"\n",
    "        analytical_var = (\n",
    "            1 - (-self._indefinite_int(t) + self._indefinite_int(self.t0)).exp()\n",
    "        )\n",
    "        return analytical_var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def analytical_sample(self, t, x_t0):\n",
    "        \"\"\"Sample Gaussian distribution given\n",
    "\n",
    "        - mean: computed from `self.analytical_mean`\n",
    "        - variance: computed from `self.analytical_var`\n",
    "        \"\"\"\n",
    "\n",
    "        mean = self.analytical_mean(t, x_t0)\n",
    "        var = self.analytical_var(t, x_t0)\n",
    "        return mean + torch.randn_like(mean) * fill_tail_dims(var.sqrt(), mean)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def analytical_score(self, x_t, t, x_t0):\n",
    "        r\"\"\"As p(X_t|X_0) is a Gaussian distribution $\\mathcal{N}(m, v)$\n",
    "\n",
    "        Its score $\\nabla log p(X_t|X_0)$ is\n",
    "\n",
    "            (x-m) / v\n",
    "        \"\"\"\n",
    "        mean = self.analytical_mean(t, x_t0)\n",
    "        var = self.analytical_var(t, x_t0)\n",
    "        return -(x_t - mean) / fill_tail_dims(var, mean).clamp_min(1e-5)\n",
    "\n",
    "    def f(self, t, y):\n",
    "        \"\"\"Modified drift according to white noise\n",
    "\n",
    "        See Eq. 14 in the paper\n",
    "        \"\"\"\n",
    "        mean_dt, var_sqrt = self.white_noise(t)\n",
    "        var_sqrt = var_sqrt.clamp(1e-5)\n",
    "\n",
    "        f = self.original_f(t, y)\n",
    "        g = self.original_g(t, y)\n",
    "\n",
    "        ret_f = (\n",
    "            f * var_sqrt.squeeze().pow(2)\n",
    "            + g * mean_dt.unsqueeze(-1).unsqueeze(-1) / self.white_noise.dt\n",
    "        )\n",
    "        return ret_f\n",
    "\n",
    "    def g(self, t, y):\n",
    "        \"\"\"Modified diffusion according to white noise\n",
    "\n",
    "        See Eq. 14 in the paper\n",
    "        \"\"\"\n",
    "        _, var_sqrt = self.white_noise(t)\n",
    "        var_sqrt = var_sqrt.clamp(1e-5)\n",
    "        g = self.original_g(t, y)\n",
    "        return g * var_sqrt\n",
    "\n",
    "    def f_and_g(self, t, y):\n",
    "        \"\"\"Modified both drift and diffusion\n",
    "\n",
    "        See Eq. 14 in the paper\n",
    "        \"\"\"\n",
    "        mean_dt, var_sqrt = self.white_noise(t)\n",
    "        var_sqrt = var_sqrt.clamp(1e-5)\n",
    "\n",
    "        f = self.original_f(t, y)\n",
    "        g = self.original_g(t, y)\n",
    "\n",
    "        ret_f = f * var_sqrt.pow(2) + g * mean_dt / self.white_noise.dt\n",
    "        return ret_f, g * var_sqrt\n",
    "\n",
    "    def sample_t1_marginal(self, batch_size, tau=1.0):\n",
    "        r\"\"\"Sample noise at time T\n",
    "        P(T) \\sim \\mathcal{N}(0, I)\n",
    "        \"\"\"\n",
    "        return torch.randn(\n",
    "            size=(batch_size, *self.input_size), device=self.device\n",
    "        ) * math.sqrt(tau)\n",
    "\n",
    "    def lambda_t(self, t):\n",
    "        \"\"\"Weight in the loss function\"\"\"\n",
    "        return self.analytical_var(t, None)\n",
    "\n",
    "    def forward(self, x_t0, partitions=1):\n",
    "        \"\"\"Compute the score matching objective.\n",
    "        Split [t0, t1] into partitions; sample uniformly on each partition to reduce gradient variance.\n",
    "        \"\"\"\n",
    "        u = torch.rand(\n",
    "            size=(x_t0.shape[0], partitions), dtype=x_t0.dtype, device=x_t0.device\n",
    "        )\n",
    "        u.mul_((self.t1 - self.t0) / partitions)\n",
    "        shifts = torch.arange(0, partitions, device=x_t0.device, dtype=x_t0.dtype)[\n",
    "            None, :\n",
    "        ]\n",
    "        shifts.mul_((self.t1 - self.t0) / partitions).add_(self.t0)\n",
    "        t = (u + shifts).reshape(-1)\n",
    "        lambda_t = self.lambda_t(t)\n",
    "\n",
    "        x_t0 = x_t0.repeat_interleave(partitions, dim=0)\n",
    "\n",
    "        # resample inducing points and recompute the sample\n",
    "        self.precompute_white_noise(batch_size=x_t0.size(0))\n",
    "        x_t = self.analytical_sample(t, x_t0)\n",
    "        fake_score = self.score(t, x_t)\n",
    "        true_score = self.analytical_score(x_t, t, x_t0)\n",
    "\n",
    "        # score matching loss\n",
    "        loss = lambda_t * ((fake_score - true_score) ** 2).flatten(start_dim=1).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ReverseDiffeqWrapper(Module):\n",
    "    \"\"\"\n",
    "    This class from `torchsde` without any changes\n",
    "    \"\"\"\n",
    "\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"stratonovich\"\n",
    "\n",
    "    def __init__(self, module: ScoreMatchingSDE):\n",
    "        super(ReverseDiffeqWrapper, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    # --- odeint ---\n",
    "    def forward(self, t, y):\n",
    "        return -(\n",
    "            self.module.f(-t, y)\n",
    "            - 0.5 * self.module.g(-t, y) ** 2 * self.module.score(-t, y)\n",
    "        )\n",
    "\n",
    "    # --- sdeint ---\n",
    "    def f(self, t, y):\n",
    "        y = y.view(-1, *self.module.input_size)\n",
    "        out = -(\n",
    "            self.module.f(-t, y) - self.module.g(-t, y) ** 2 * self.module.score(-t, y)\n",
    "        )\n",
    "        return out.flatten(start_dim=1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        y = y.view(-1, *self.module.input_size)\n",
    "        out = -self.module.g(-t, y)\n",
    "        return out.flatten(start_dim=1)\n",
    "\n",
    "    # --- sample ---\n",
    "    def sample_t1_marginal(self, batch_size, tau=1.0):\n",
    "        return self.module.sample_t1_marginal(batch_size, tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ode_sample(self, batch_size=64, tau=1.0, t=None, y=None, dt=1e-2):\n",
    "        self.module.eval()\n",
    "        self.module.precompute_white_noise(batch_size=batch_size)\n",
    "        t = torch.tensor([-self.t1, -self.t0], device=self.device) if t is None else t\n",
    "        y = self.sample_t1_marginal(batch_size, tau) if y is None else y\n",
    "        return torchdiffeq.odeint(self, y, t, method=\"rk4\", options={\"step_size\": dt})\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ode_sample_final(self, batch_size=64, tau=1.0, t=None, y=None, dt=1e-2):\n",
    "        return self.ode_sample(batch_size, tau, t, y, dt)[-1]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sde_sample(\n",
    "        self, batch_size=64, tau=1.0, t=None, y=None, dt=1e-2, tweedie_correction=True\n",
    "    ):\n",
    "        self.module.eval()\n",
    "\n",
    "        t = torch.tensor([-self.t1, -self.t0], device=self.device) if t is None else t\n",
    "        y = self.sample_t1_marginal(batch_size, tau) if y is None else y\n",
    "        self.module.precompute_white_noise(batch_size)\n",
    "        ys = torchsde.sdeint(self, y.flatten(start_dim=1), t, dt=dt)\n",
    "        ys = ys.view(len(t), *y.size())\n",
    "        if tweedie_correction:\n",
    "            ys[-1] = self.tweedie_correction(self.t0, ys[-1], dt)\n",
    "        return ys\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sde_sample_final(self, batch_size=64, tau=1.0, t=None, y=None, dt=1e-2):\n",
    "        return self.sde_sample(batch_size, tau, t, y, dt)[-1]\n",
    "\n",
    "    def tweedie_correction(self, t, y, dt):\n",
    "        return y + dt**2 * self.module.score(t, y)\n",
    "\n",
    "    @property\n",
    "    def t0(self):\n",
    "        return self.module.t0\n",
    "\n",
    "    @property\n",
    "    def t1(self):\n",
    "        return self.module.t1\n",
    "\n",
    "\n",
    "def preprocess(x, logit_transform, alpha=0.95):\n",
    "    if logit_transform:\n",
    "        x = alpha + (1 - 2 * alpha) * x\n",
    "        x = (x / (1 - x)).log()\n",
    "    else:\n",
    "        x = (x - 0.5) * 2\n",
    "    return x\n",
    "\n",
    "\n",
    "def postprocess(x, logit_transform, alpha=0.95, clamp=True):\n",
    "    if logit_transform:\n",
    "        x = (x.sigmoid() - alpha) / (1 - 2 * alpha)\n",
    "    else:\n",
    "        x = x * 0.5 + 0.5\n",
    "    return x.clamp(min=0.0, max=1.0) if clamp else x\n",
    "\n",
    "\n",
    "def make_loader(\n",
    "    root=\"./data/mnist\",\n",
    "    train_batch_size=128,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "):\n",
    "    \"\"\"Make a simple loader for training images in MNIST.\"\"\"\n",
    "\n",
    "    def dequantize(x, nvals=256):\n",
    "        \"\"\"[0, 1] -> [0, nvals] -> add uniform noise -> [0, 1]\"\"\"\n",
    "        noise = x.new().resize_as_(x).uniform_()\n",
    "        x = x * (nvals - 1) + noise\n",
    "        x = x / nvals\n",
    "        return x\n",
    "\n",
    "    train_transform = tv.transforms.Compose([tv.transforms.ToTensor(), dequantize])\n",
    "    train_data = tv.datasets.MNIST(\n",
    "        root, train=True, transform=train_transform, download=True\n",
    "    )\n",
    "    train_loader = data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=train_batch_size,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_data = tv.datasets.MNIST(\n",
    "        root, train=False, transform=train_transform, download=False\n",
    "    )\n",
    "\n",
    "    test_loader = data.DataLoader(test_data, batch_size=train_batch_size)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#   White noise sampler\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "class ConstantHurst(SparseGPNoise):\n",
    "    def __init__(\n",
    "        self, t0, t1, dt, H=0.3, dim=1, num_steps=100, num_inducings=10\n",
    "    ) -> None:\n",
    "        super().__init__(t0, t1, dt, dim, num_steps, num_inducings)\n",
    "        self.H = H\n",
    "\n",
    "        # make different inducing points\n",
    "        del self.Z\n",
    "        Z_ = torch.linspace(1.1, 2.0, self.num_inducing)\n",
    "        self.register_buffer(\"Z\", Z_)\n",
    "\n",
    "    def compute_hurst(self, t):\n",
    "        t = t.view(-1, 1)\n",
    "        return torch.ones_like(t) * self.H\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Fractional Brownian Motion H={self.H}\"\n",
    "\n",
    "\n",
    "class LinearHurstWhiteNoise(SparseGPNoise):\n",
    "    def __init__(self, t0, t1, dt, ht0=0.3, ht1=0.8, num_steps=100) -> None:\n",
    "        super().__init__(t0, t1, dt, num_steps=num_steps)\n",
    "        self.ht0, self.ht1 = ht0, ht1\n",
    "\n",
    "        # make different inducing points outside of the bound\n",
    "        del self.Z\n",
    "        Z_ = torch.linspace(1.1, 2.0, self.num_inducing)\n",
    "        self.register_buffer(\"Z\", Z_)\n",
    "\n",
    "    def compute_hurst(self, t):\n",
    "        ret = self.ht0 + (t - self.t0) * (self.ht1 - self.ht0) / (self.t1 - self.t0)\n",
    "        ret = ret.view(-1, 1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Multifractional Brownian Motion. \\\n",
    "            Linear h(t): ht0={self.ht0}, ht1={self.ht1}, t0,t1={self.t0},{self.t1}\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#   Helper function compute negative log likelihood\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "def hutch_trace(x_out, x_in, noise=None):\n",
    "    \"\"\"Hutchinson's trick computing the trace of a matrix\"\"\"\n",
    "    jvp = torch.autograd.grad(x_out, x_in, noise, create_graph=False)[0]\n",
    "    trace = torch.einsum(\"abcd,abcd->a\", jvp, noise)\n",
    "    return trace\n",
    "\n",
    "\n",
    "def compute_ll(sde: ScoreMatchingSDE, data, offset, dt=1e-2):\n",
    "    \"\"\"Compute log likelihood of a small batch of data\"\"\"\n",
    "    N = np.prod(data.size()[1:])\n",
    "    data = data.view(-1, N)\n",
    "    init = torch.cat([torch.zeros(data.size(0), 1).to(data), data], dim=-1)\n",
    "    t0, t1 = sde.t0, sde.t1\n",
    "    t = torch.tensor([t0 + 1e-5, t1], device=sde.device)\n",
    "    noise = torch.randn(size=(data.size(0), *sde.input_size)).to(sde.device)\n",
    "\n",
    "    def vector_field(t, y):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            y_in = y[:, 1:].detach().requires_grad_(True)\n",
    "            y_in = y_in.view(-1, *sde.input_size)\n",
    "            sde.precompute_white_noise(batch_size=y_in.size(0))\n",
    "            y_out = sde.f(t, y_in) - 0.5 * sde.g(t, y_in) * sde.score(t, y_in)\n",
    "            trace = hutch_trace(y_out, y_in, noise)\n",
    "\n",
    "        return torch.cat([trace.unsqueeze(-1), y_out.view(-1, N)], dim=-1)\n",
    "\n",
    "    ys = torchdiffeq.odeint(\n",
    "        vector_field, init, t, method=\"euler\", options={\"step_size\": dt}\n",
    "    )\n",
    "    ys = ys[-1]  # batch x 785\n",
    "    delta_lprob = ys[:, 0]\n",
    "    z = ys[:, 1:]\n",
    "    lprob = -0.5 * (z**2).sum(-1) - 0.5 * 784 * np.log(2 * np.pi)\n",
    "    bit_per_dim = -(delta_lprob.squeeze() + lprob.squeeze())\n",
    "    bit_per_dim = bit_per_dim / z.size(-1) / np.log(2.0)\n",
    "    bit_per_dim += offset\n",
    "    # equation 27 in (https://arxiv.org/pdf/1705.07057.pdf)\n",
    "    last_term = (\n",
    "        torch.log2(torch.sigmoid(z) + 1e-12)\n",
    "        + torch.log2(1.0 - torch.sigmoid(z) + 1e-12)\n",
    "    ).sum(dim=-1) / N\n",
    "    bit_per_dim += last_term\n",
    "    return bit_per_dim.sum(dim=0).cpu().data\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "#   MAIN\n",
    "# -------------------------------------------------------\n",
    "def main(\n",
    "    device=\"cuda\",\n",
    "    train_dir=\"./dump/score-based\",\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    batch_size=128,\n",
    "    pause_every=1000,\n",
    "    tau=1.0,\n",
    "    logit_transform=True,\n",
    "    alpha=0.05,\n",
    "    wandb_mode=\"offline\",\n",
    "):\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data.\n",
    "    train_loader, eval_loader = make_loader(\n",
    "        root=os.path.join(train_dir, \"data\"), train_batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Model + optimizer.\n",
    "    denoiser = unet.Unet(\n",
    "        input_size=(1, 28, 28),\n",
    "        dim_mults=(\n",
    "            1,\n",
    "            2,\n",
    "            4,\n",
    "        ),\n",
    "        attention_cls=unet.LinearTimeSelfAttention,\n",
    "    )\n",
    "\n",
    "    # TODO: commment and uncomment the following to try out different settings\n",
    "\n",
    "    # constant Hurst\n",
    "    # white_noise = ConstantHurst(t0=0.,\n",
    "    #                             t1=2.1,\n",
    "    #                             dt=1e-2,\n",
    "    #                             H=0.3).to(device)\n",
    "\n",
    "    # linearly increasing Hurst\n",
    "    # white_noise = LinearHurstWhiteNoise(t0=0.,\n",
    "    #                                     t1=2.1,\n",
    "    #                                     dt=1e-2,\n",
    "    #                                     ht0=0.2,\n",
    "    #                                     ht1=0.7).to(device)\n",
    "\n",
    "    # linearly decreasing Hurst\n",
    "    white_noise = LinearHurstWhiteNoise(t0=0.0, t1=2.1, dt=1e-2, ht0=0.8, ht1=0.1).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    forward = ScoreMatchingSDE(denoiser=denoiser, white_noise=white_noise).to(device)\n",
    "    forward.init()\n",
    "    reverse = ReverseDiffeqWrapper(forward)\n",
    "    optimizer = optim.Adam(params=forward.parameters(), lr=lr)\n",
    "\n",
    "    # equation 27 in (https://arxiv.org/pdf/1705.07057.pdf)\n",
    "    bpd_offset = -np.log2(1 - 2 * alpha) + 8.0\n",
    "\n",
    "    x, _ = next(iter(eval_loader))\n",
    "    x = preprocess(x.to(device), logit_transform=logit_transform, alpha=alpha)\n",
    "    nll_batch = compute_ll(forward, x, offset=bpd_offset)\n",
    "\n",
    "    def plot(imgs, path):\n",
    "        assert not torch.any(torch.isnan(imgs)), \"Found nans in images\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        imgs = (\n",
    "            postprocess(imgs, logit_transform=logit_transform, alpha=alpha)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "        )\n",
    "        tv.utils.save_image(imgs, path)\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"score-based-nll\",\n",
    "        config={\"white noise\": white_noise.__repr__()},\n",
    "        mode=wandb_mode,\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        for x, _ in tqdm.tqdm(train_loader):\n",
    "            forward.train()\n",
    "            forward.zero_grad()\n",
    "            x = preprocess(x.to(device), logit_transform=logit_transform, alpha=alpha)\n",
    "            loss = forward(x).mean(dim=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            wandb.log({\"loss\": loss.detach().cpu().numpy()})\n",
    "\n",
    "            if global_step % pause_every == 0:\n",
    "                logging.warning(f\"global_step: {global_step:06d}, loss: {loss:.4f}\")\n",
    "\n",
    "                img_path = os.path.join(\n",
    "                    train_dir, \"ode_samples\", f\"global_step_{global_step:07d}.png\"\n",
    "                )\n",
    "                ode_samples = reverse.ode_sample_final(tau=tau)\n",
    "                plot(ode_samples, img_path)\n",
    "                wandb.log({\"ODE-sample\": wandb.Image(img_path)})\n",
    "\n",
    "                img_path = os.path.join(\n",
    "                    train_dir, \"sde_samples\", f\"global_step_{global_step:07d}.png\"\n",
    "                )\n",
    "                sde_samples = reverse.sde_sample_final(tau=tau)\n",
    "                plot(sde_samples, img_path)\n",
    "                wandb.log({\"SDE-sample\": wandb.Image(img_path)})\n",
    "\n",
    "                # get the first batch of eval\n",
    "                x, _ = next(iter(eval_loader))\n",
    "                x = preprocess(\n",
    "                    x.to(device), logit_transform=logit_transform, alpha=alpha\n",
    "                )\n",
    "                nll_batch = compute_ll(forward, x, offset=bpd_offset)\n",
    "                wandb.log({\"nll_batch\": nll_batch / batch_size})\n",
    "\n",
    "        if epoch == epochs // 2:\n",
    "            print(\"Evaluating negative log likelihood\")\n",
    "            nll = 0.0\n",
    "            for x, _ in tqdm.tqdm(eval_loader):\n",
    "                x = preprocess(\n",
    "                    x.to(device), logit_transform=logit_transform, alpha=alpha\n",
    "                )\n",
    "                nll += compute_ll(forward, x, offset=bpd_offset)\n",
    "            wandb.log({\"NLL\": nll / 10000.0})\n",
    "\n",
    "    print(\"Final evaluating negative log likelihood\")\n",
    "    nll = 0.0\n",
    "    for x, _ in tqdm.tqdm(eval_loader):\n",
    "        x = preprocess(x.to(device), logit_transform=logit_transform, alpha=alpha)\n",
    "        nll += compute_ll(forward, x, offset=bpd_offset)\n",
    "\n",
    "    wandb.log({\"NLL\": nll / 10000.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60821819-907c-40bc-b332-fdcec5b0f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dump/score-based/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 29580221.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dump/score-based/data/MNIST/raw/train-images-idx3-ubyte.gz to ./dump/score-based/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dump/score-based/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 81848441.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dump/score-based/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./dump/score-based/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dump/score-based/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 32021573.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dump/score-based/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dump/score-based/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dump/score-based/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 18335446.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dump/score-based/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dump/score-based/data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [2:01:44<00:00, 15.61s/it]    \n",
      "  2%|▏         | 8/468 [01:07<1:04:40,  8.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 555\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(device, train_dir, epochs, lr, batch_size, pause_every, tau, logit_transform, alpha, wandb_mode)\u001b[0m\n\u001b[1;32m    553\u001b[0m x \u001b[38;5;241m=\u001b[39m preprocess(x\u001b[38;5;241m.\u001b[39mto(device), logit_transform\u001b[38;5;241m=\u001b[39mlogit_transform, alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[1;32m    554\u001b[0m loss \u001b[38;5;241m=\u001b[39m forward(x)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 555\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    557\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8a39b-0b68-4582-9716-74fe831857c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
